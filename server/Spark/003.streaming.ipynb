{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff867267-cd24-4269-9193-2227c5c1cf13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SPARK STREAMING\n",
    "\n",
    "Aache Spark Streaming es un componente del ecosistema Apache Spark diseñado específicamente para el procesamiento de datos en tiempo real y el análisis de datos de flujo continuo. Permite a los desarrolladores y analistas de datos procesar datos en tiempo real de manera escalable, tolerante a fallos y de alto rendimiento utilizando el modelo de programación familiar de Apache Spark.\n",
    "\n",
    "Para hacer esta demo, realizar los siguientes pasos:\n",
    "\n",
    "1. Tener la imagen de spark corriendo y el contenedor anclado el VScode\n",
    "2. abrir una terminal, y ejecutar `nc -l -k 12345`\n",
    "3. ir al notebook y ejecutar todas las secuencias de spark\n",
    "4. volver a la consola y enviar mensajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca8173d-6bfe-4ff8-b5c9-62dad2df49d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CONFIGURAR SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7122ce-c14d-4f86-8f72-a47ce9e6c5af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b4172dc78519:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>unal streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fddb802f9a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"unal streaming\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5031ca-22c4-49bc-bf96-0772b8b6563e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CONFIGURAR FUENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5959abe-6cfd-44d0-b8c9-19d749cde238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", \"12345\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b1ee73-48ca-4211-abff-c769dfc45ff9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CONFIGURAR FUNCION DE TRANSFORMACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08564f9d-4338-444d-8a47-dcb15a871954",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funcion para realizar transformaciones\n",
    "def process_word_count(streaming_df):\n",
    "    # lee y aplica transformacion\n",
    "    words_df = streaming_df.selectExpr(\"explode(split(value, ' ')) as word\")\n",
    "\n",
    "    # Arealiza proceso de agregación\n",
    "    agg_words_df = words_df \\\n",
    "        .groupBy(\"word\") \\\n",
    "        .agg(count(\"word\").alias(\"count\"))\n",
    "    \n",
    "    # imprimir esquema\n",
    "    agg_words_df.printSchema()\n",
    "    return agg_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f779dd9e-a5c8-4c45-944e-39ac289a1d64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ALMACENAMIENTO\n",
    "\n",
    "para ver como configurar mas destinos, mirar:\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b93a92-d6a2-44ae-b5fb-ac1798b138cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lee y transforma\n",
    "agg_words_df = process_word_count(streaming_df)\n",
    "\n",
    "# escritura en consola\n",
    "writing_df = agg_words_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "# de este modo, se ejecuta \n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3470490410964087,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "003.streaming",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
